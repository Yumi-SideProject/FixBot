import json
import time
import re
import numpy as np
import faiss  # ğŸ”¥ FAISS ì¶”ê°€ (pip install faiss-cpu)
from groq import Groq  # ğŸ”¥ Groq API ì‚¬ìš©
from rank_bm25 import BM25Okapi  # ğŸ”¥ BM25 ì¶”ê°€ (pip install rank-bm25)
from sentence_transformers import SentenceTransformer  # ğŸ”¥ FAISS ë²¡í„°í™” ëª¨ë¸ (pip install sentence-transformers)

# âœ… Groq API ì„¤ì •
API_KEY = "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"  # ğŸ”¥ ì—¬ê¸°ì— Groq API Key ì…ë ¥
client = Groq(api_key=API_KEY)  # ğŸ”¥ API Keyë¥¼ ì‚¬ìš©í•˜ì—¬ Groq í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”

# âœ… ëª¨ë¸ ì •ë³´
MODEL_NAME = "llama-3.3-70b-versatile"

# âœ… FAISS ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì •í™•ë„ê°€ ë†’ì€ ëª¨ë¸ ì‚¬ìš©)
embedding_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")  # ğŸ”¥ ì •í™•ë„ ë†’ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©

# âœ… íŒŒì¼ ê²½ë¡œ
SENTENCES_FILES = ["/content/FixBot/Samsung_sentences_1.json", "/content/FixBot/Samsung_sentences_2.json"]
QUESTIONS_FILE = "/content/FixBot/Samsung_cleaned_questions.txt"
OUTPUT_FILE = "/content/FixBot/Samsung_answers.json"

# âœ… ë¬¸ì¥ ë°ì´í„° ë¡œë“œ
def load_sentences():
    """9000ê°œ ë¬¸ì¥ì„ ë¡œë“œí•˜ì—¬ BM25 + FAISS ì¸ë±ìŠ¤ ìƒì„±"""
    sentences = []
    for file_path in SENTENCES_FILES:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            sentences.extend([item["sentence"] for item in data])
    return sentences

# âœ… ì§ˆë¬¸ ë°ì´í„° ë¡œë“œ
def load_questions():
    with open(QUESTIONS_FILE, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines() if line.strip()]

# âœ… `BM25` ì¸ë±ìŠ¤ ìƒì„±
def create_bm25_index(sentences):
    """BM25ë¥¼ ìœ„í•œ ë¬¸ì¥ ì¸ë±ìŠ¤ ìƒì„±"""
    tokenized_sentences = [re.findall(r'\w+', s.lower()) for s in sentences]  # ğŸ”¥ í† í°í™”
    return BM25Okapi(tokenized_sentences), tokenized_sentences

# âœ… `FAISS` ì¸ë±ìŠ¤ ìƒì„±
def create_faiss_index(sentences):
    sentence_embeddings = embedding_model.encode(sentences, convert_to_numpy=True)
    d = sentence_embeddings.shape[1]

    index = faiss.IndexFlatL2(d)  # L2 ê±°ë¦¬ ê¸°ë°˜ ê²€ìƒ‰
    index.add(sentence_embeddings)

    # ğŸ”¥ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ (íƒìƒ‰ ë°˜ê²½ í™•ì¥)
    index.nprobe = 20
    return index, sentence_embeddings

# âœ… `BM25 + FAISS` ê¸°ë°˜ ê´€ë ¨ ë¬¸ì¥ ì°¾ê¸°
def find_relevant_sentences(question, bm25, sentences, tokenized_sentences, faiss_index, sentence_embeddings, top_n=10):
    """BM25ì™€ FAISSë¥¼ ê²°í•©í•˜ì—¬ ê´€ë ¨ ë¬¸ì¥ ê²€ìƒ‰"""
    
    # ğŸ”¥ 1ï¸âƒ£ BM25 ê²€ìƒ‰ (ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œê°€ ìœ ì‚¬í•œ ìƒìœ„ 100ê°œ ë¬¸ì¥ ì„ íƒ)
    tokenized_query = re.findall(r'\w+', question.lower())
    bm25_scores = bm25.get_scores(tokenized_query)
    top_bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:100]
    bm25_selected_sentences = [sentences[i] for i in top_bm25_indices]

    if not bm25_selected_sentences:
        print(f"âš ï¸ BM25ì—ì„œ ê´€ë ¨ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {question}")
        return []

    # ğŸ”¥ 2ï¸âƒ£ FAISS ê²€ìƒ‰ (BM25 ìƒìœ„ 100ê°œ ë¬¸ì¥ ì¤‘ ê°€ì¥ ìœ ì‚¬í•œ 10ê°œ ë¬¸ì¥ ì„ íƒ)
    query_embedding = embedding_model.encode([question], convert_to_numpy=True)
    _, faiss_indices = faiss_index.search(query_embedding, top_n)

    if len(faiss_indices[0]) == 0:
        print(f"âš ï¸ FAISSì—ì„œ ê´€ë ¨ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {question}")
        
        # ğŸ”¥ FAISSê°€ ì‹¤íŒ¨í–ˆì„ ê²½ìš°, BM25 ê²°ê³¼ë¥¼ FAISS ì¸ë±ìŠ¤ì— ì¶”ê°€ í›„ ì¬ê²€ìƒ‰
        faiss_index.add(embedding_model.encode(bm25_selected_sentences, convert_to_numpy=True))
        _, faiss_indices = faiss_index.search(query_embedding, top_n)

        if len(faiss_indices[0]) == 0:
            print(f"âš ï¸ FAISS ì¬ê²€ìƒ‰ ì‹¤íŒ¨, BM25 ë¬¸ì¥ ì‚¬ìš©: {question}")
            return bm25_selected_sentences[:top_n]  # BM25 ê²°ê³¼ ë°˜í™˜

    # ğŸ”¥ ìµœì¢… ë¬¸ì¥ ì„ íƒ
    final_selected_sentences = [bm25_selected_sentences[i] for i in faiss_indices[0] if i < len(bm25_selected_sentences)]
    
    if not final_selected_sentences:
        print(f"âš ï¸ FAISS ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, BM25 ë¬¸ì¥ ì‚¬ìš©: {question}")
        return bm25_selected_sentences[:top_n]

    return final_selected_sentences

# âœ… Groq APIë¥¼ ì´ìš©í•œ ë‹µë³€ ìƒì„± (LLM í”„ë¡¬í”„íŠ¸ ê°œì„ )
def generate_answer(question, relevant_sentences):
    """Groq APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
    if not relevant_sentences:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°ì— ë¬¸ì˜í•˜ì„¸ìš”."

    # ğŸ”¥ LLM í”„ë¡¬í”„íŠ¸ ê°œì„  (ì§ì ‘ì ì¸ ë‹µë³€ ìœ ë„)
    prompt = f"""
    ## ë„ˆì˜ ëª©ì :
    - **ê³ ê°ë“¤ì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤.**
    - **ê³ ê°ë“¤ì´ ì²˜í•œ ë¬¸ì œ ìƒí™©ì—ì„œ ì ì ˆí•œ í•´ê²°ì±…ì„ ì •í™•í•œ ì¶œì²˜/ìë£Œ ê¸°ë°˜ìœ¼ë¡œ ì œì‹œí•œë‹¤.**

    ## ì§ˆë¬¸:
    {question}

    ## ê´€ë ¨ ë¬¸ì„œ:
    {' '.join(relevant_sentences)}

    ## ë‹µë³€:
    - **ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í•´ê²° ë°©ë²• ìì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.**
    - **ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**
    - **ì •í™•í•˜ê³  ëª…í™•í•œ í•´ê²° ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.**
    - **ì£¼ë¶€ ê³ ê°ë“¤ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.**
    - **ê³ ê°ë“¤ì´ ê°€ì¥ ì‰½ê³  í¸í•˜ê²Œ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ìš°ì„ ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.**
    """

    response = client.chat.completions.create(
        model=MODEL_NAME,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,  # ğŸ”¥ ë‹µë³€ì„ ë” ì •í™•í•˜ê²Œ
        max_tokens=2000,  # ğŸ”¥ ë„ˆë¬´ ê¸´ ë‹µë³€ ë°©ì§€
        top_p=0.85,  # ğŸ”¥ í’ˆì§ˆ ìµœì í™”
        stream=False
    )

    return response.choices[0].message.content.strip()

# âœ… ì „ì²´ ì§ˆë¬¸ ì²˜ë¦¬ ë° ë‹µë³€ ìƒì„±
def process_questions():
    sentences = load_sentences()
    questions = load_questions()

    # ğŸ”¥ `BM25` + `FAISS` ì¸ë±ìŠ¤ ìƒì„±
    bm25, tokenized_sentences = create_bm25_index(sentences)
    faiss_index, sentence_embeddings = create_faiss_index(sentences)

    answers = []
    for idx, question in enumerate(questions):
        print(f"ğŸ“ ì§ˆë¬¸ {idx+1}/{len(questions)}: {question}")

        # ê´€ë ¨ ë¬¸ì¥ ì°¾ê¸° (`BM25 + FAISS` í™œìš©)
        relevant_sentences = find_relevant_sentences(question, bm25, sentences, tokenized_sentences, faiss_index, sentence_embeddings)

        # LLMì„ í™œìš©í•´ ë‹µë³€ ìƒì„±
        answer = generate_answer(question, relevant_sentences)
        print('--------------')
        print(answer)
        print('--------------')

        # ê²°ê³¼ ì €ì¥
        answers.append({"question": question, "answer": answer})

        # Rate limit ë°©ì§€
        time.sleep(2)

    # JSONìœ¼ë¡œ ì €ì¥
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(answers, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… ë‹µë³€ ì €ì¥ ì™„ë£Œ: {OUTPUT_FILE}")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    process_questions()
