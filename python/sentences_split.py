import json
import re
import nltk
from nltk.tokenize import sent_tokenize
import unicodedata

# NLTK ë¬¸ì¥ ë¶„í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ë¡œë“œ
nltk.download('punkt')

# ğŸ“Œ JSON íŒŒì¼ ë¡œë“œ
json_path = "C:/Users/tyumi/Desktop/side_pjt/stt/stt_results.json"
with open(json_path, "r", encoding="utf-8") as file:
    data = json.load(file)

# ğŸ“Œ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±° (ì˜¤ë¥˜ ë©”ì‹œì§€, ì‹œìŠ¤í…œ ë¡œê·¸, íŒŒì¼ ê²½ë¡œ ë“±)
def is_valid_sentence(sentence):
    # ì‹œìŠ¤í…œ ì˜¤ë¥˜ ê´€ë ¨ ë¬¸ì¥ í•„í„°ë§
    error_patterns = [
        r"Skipping .* due to OSError",  # ì˜ˆ: "Skipping /path/file.mp3 due to OSError"
        r"Operation not supported",  # ì˜ˆ: "Operation not supported"
        r"Traceback.*",  # ì˜ˆ: Traceback ì—ëŸ¬ ë©”ì‹œì§€
        r"File \".*\", line \d+",  # ì˜ˆ: "File 'script.py', line 23"
        r"SyntaxError",  # íŒŒì´ì¬ ì—ëŸ¬ ë©”ì‹œì§€
        r"OSError",  # OS ê´€ë ¨ ì—ëŸ¬ ë©”ì‹œì§€
        r"Permission denied",  # ì ‘ê·¼ ê¶Œí•œ ì˜¤ë¥˜
        r"Downloading:",  # yt-dlp ë‹¤ìš´ë¡œë“œ ë©”ì‹œì§€
        r"Converting:",  # ë³€í™˜ ê³¼ì •ì—ì„œ ì¶œë ¥ë˜ëŠ” ë¡œê·¸
        r"File saved at",  # íŒŒì¼ ì €ì¥ ë¡œê·¸
        r"Error:",  # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë¬¸êµ¬
        r"stderr",  # ì‹œìŠ¤í…œ ë¡œê·¸ ê´€ë ¨
        r"stdout",  # ì‹œìŠ¤í…œ ë¡œê·¸ ê´€ë ¨
    ]
    for pattern in error_patterns:
        if re.search(pattern, sentence, re.IGNORECASE):
            return False  # ì˜¤ë¥˜ ë¬¸ì¥ ì œê±°
    return True

# ğŸ“Œ ê¹¨ì§„ ë¬¸ì ë° ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
def clean_text(text):
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (ë³µì¡í•œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì¼ë°˜ ë¬¸ìë¡œ ë³€í™˜)
    text = unicodedata.normalize("NFKC", text)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±° [00:00.000 --> 00:03.200] í˜•ì‹
    text = re.sub(r'\[\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}\.\d{3}\]', '', text)

    # ë¶ˆí•„ìš”í•œ ê´‘ê³  ë¬¸êµ¬ ì œê±°
    remove_phrases = [
        "êµ¬ë…ê³¼ ì¢‹ì•„ìš” ë¶€íƒë“œë¦½ë‹ˆë‹¤", "êµ¬ë…", "ì¢‹ì•„ìš”", "ì‹œì²­í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
        "ì˜ìƒ ëê¹Œì§€ ì‹œì²­í•´ì£¼ì„¸ìš”", "ì•ŒëŒ ì„¤ì •", "ì±„ë„", "ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì„¸ìš”"
    ]
    for phrase in remove_phrases:
        text = text.replace(phrase, '')

    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ì£¼ìš” êµ¬ë‘ì  ì œì™¸)
    text = re.sub(r'[^ê°€-í£A-Za-z0-9.,!?\'"~@#$%^&*()<>;:/+=_\-]', ' ', text)

    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# ğŸ“Œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ë° ì •ì œ
cleaned_data = []
for item in data:
    text = item.get("transcript", "").strip()  # í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°

    # 1ï¸âƒ£ ë‹¨ë½ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    paragraphs = text.split("\n")

    for para in paragraphs:
        para = clean_text(para)  # í…ìŠ¤íŠ¸ ì •ì œ
        if not para or len(para) < 5:  # ë¹ˆ ë¬¸ìì—´ ë° ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
            continue

        # 2ï¸âƒ£ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‹¤ì‹œ ë¶„í• 
        sentences = sent_tokenize(para)

        for sentence in sentences:
            if len(sentence) > 10 and is_valid_sentence(sentence):  # ì˜¤ë¥˜ ë©”ì‹œì§€ í•„í„°ë§
                cleaned_data.append({"sentence": sentence})  # ê°œë³„ ë¬¸ì¥ ì €ì¥

# ğŸ“Œ ì •ì œëœ JSON ë°ì´í„° ì €ì¥
output_path = "C:/Users/tyumi/Desktop/side_pjt/stt/cleaned_sentences.json"
with open(output_path, "w", encoding="utf-8") as out_file:
    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)

print(f"âœ… ë¬¸ì¥ ì •ì œ ì™„ë£Œ! íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
