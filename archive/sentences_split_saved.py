import json
import re
import nltk
from nltk.tokenize import sent_tokenize
import unicodedata

# NLTK ë¬¸ì¥ ë¶„í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë‹¤ìš´ë¡œë“œ
nltk.download('punkt')

# ğŸ“Œ JSON íŒŒì¼ ë¡œë“œ
json_path = "C:/Users/tyumi/Desktop/side_pjt/stt/stt_results.json"
with open(json_path, "r", encoding="utf-8") as file:
    data = json.load(file)

# ğŸ“Œ ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±° (ì˜¤ë¥˜ ë©”ì‹œì§€, ì‹œìŠ¤í…œ ë¡œê·¸, íŒŒì¼ ê²½ë¡œ ë“±)
def is_valid_sentence(sentence):
    error_patterns = [
        r"Skipping .* due to OSError",
        r"Operation not supported",
        r"Traceback.*",
        r"File \".*\", line \d+",
        r"SyntaxError",
        r"OSError",
        r"Permission denied",
        r"Downloading:",
        r"Converting:",
        r"File saved at",
        r"Error:",
        r"stderr",
        r"stdout",
    ]
    for pattern in error_patterns:
        if re.search(pattern, sentence, re.IGNORECASE):
            return False
    return True

# ğŸ“Œ ê¹¨ì§„ ë¬¸ì ë° ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
def clean_text(text):
    text = unicodedata.normalize("NFKC", text)

    # íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
    text = re.sub(r'\[\d{2}:\d{2}\.\d{3} --> \d{2}:\d{2}\.\d{3}\]', '', text)

    # ê´‘ê³ ì„± ë¬¸êµ¬ ì œê±°
    remove_phrases = [
        "êµ¬ë…ê³¼ ì¢‹ì•„ìš” ë¶€íƒë“œë¦½ë‹ˆë‹¤", "êµ¬ë…", "ì¢‹ì•„ìš”", "ì‹œì²­í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤",
        "ì˜ìƒ ëê¹Œì§€ ì‹œì²­í•´ì£¼ì„¸ìš”", "ì•ŒëŒ ì„¤ì •", "ì±„ë„", "ëŒ“ê¸€ ë‚¨ê²¨ì£¼ì„¸ìš”"
    ]
    for phrase in remove_phrases:
        text = text.replace(phrase, '')

    # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ì–´, ìˆ«ì, ì£¼ìš” êµ¬ë‘ì  ì œì™¸)
    text = re.sub(r'[^ê°€-í£A-Za-z0-9.,!?\'"~@#$%^&*()<>;:/+=_\-]', ' ', text)

    # ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# ğŸ“Œ ë¬¸ì¥ ì •ì œ ë° ì €ì¥ êµ¬ì¡° ë³€ê²½ (ì¶œì²˜ URL, ì œëª©, ì •ì œëœ ë¬¸ì¥)
cleaned_data = []
for item in data:
    url = item.get("url", "")
    transcript = item.get("transcript", "").strip()
    paragraphs = transcript.split("\n")
    for para in paragraphs:
        para = clean_text(para)
        if not para or len(para) < 5:
            continue

        sentences = sent_tokenize(para)
        for sentence in sentences:
            if len(sentence) > 10 and is_valid_sentence(sentence):
                cleaned_data.append({
                    "url": url,
                    "sentence": sentence
                })

# ğŸ“Œ ì •ì œëœ JSON ë°ì´í„° ì €ì¥
output_path = "C:/Users/tyumi/Desktop/side_pjt/stt/cleaned_sentences_saved.json"
with open(output_path, "w", encoding="utf-8") as out_file:
    json.dump(cleaned_data, out_file, ensure_ascii=False, indent=4)

print(f"âœ… ë¬¸ì¥ ì •ì œ ì™„ë£Œ! íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
